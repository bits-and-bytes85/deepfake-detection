# -*- coding: utf-8 -*-
"""DeepfakeDetectionModel

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lhHqm43_0bXc9_XOf_sL6bfa9vtrXqtl

```
# This is formatted as code
```
"""

import os
import warnings
from PIL import Image
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import random
import math

from sklearn.model_selection import train_test_split

# for grayscale conversion of numpy picture array
from skimage import color

# for exploring filters of a CNN
from scipy.signal import convolve2d

# for the confusion matrix
from sklearn.metrics import confusion_matrix
import seaborn as sn

# for making a model using tensorflow
import tensorflow as tf
from keras.datasets import cifar10
from keras import Sequential, layers, callbacks
from keras.losses import SparseCategoricalCrossentropy
from keras.layers import (
    Dense,
    Conv2D,
    MaxPool2D,
    Flatten,
    Dropout,
    BatchNormalization,
)

from google.colab import drive
drive.mount('/content/gdrive')

!unzip /content/gdrive/MyDrive/DeepFake_Detection/archive.zip

df = pd.read_csv('metadata.csv')

df.head()

import cv2

len(df[df.label=='FAKE']),len(df[df.label=='REAL'])
real_df = df[df["label"] == "REAL"]
fake_df = df[df["label"] == "FAKE"]

#question: should the real and fake training sets be equal size? as is the training set has a lot more fake pictures
real_df = real_df.sample(5000, random_state=42)
fake_df = fake_df.sample(5000, random_state=42)
sample_data = pd.concat([real_df, fake_df])

# train_set, test_set = train_test_split(sample_data,test_size=0.5,random_state=42,stratify=sample_data['label'])
train_set, test_set = train_test_split(sample_data,test_size=0.2,random_state=42,stratify=sample_data['label'])

def retreive_dataset(set_name):
    images,labels=[],[]
    for (img, imclass) in zip(set_name['videoname'], set_name['label']):
        var = (cv2.imread(os.path.join('/content/faces_224/', img[:-4] + '.jpg')))
        images.append(var)
        if(imclass=='FAKE'):
            labels.append(1)
        else:
            labels.append(0)

    return np.array(images),np.array(labels)

x_train,y_train=retreive_dataset(train_set)
print(x_train[0])
print(train_set.shape)

gray_training = []
def gray_scale(set_name):
  for img in set_name:
    gray_scaled = tf.image.rgb_to_grayscale(img)
    gray_training.append(gray_scaled)
gray_scale(x_train)

plt.imshow(gray_training[102])
print(y_train[102])

# Filters

sobel_filter = np.array([[-1, 0, 1],
                        [-2, 0, 2],
                        [-1, 0, 1]])

gaussian_filter = np.array([[1, 2, 1],
                           [2, 4, 2],
                           [1, 2, 1]]) / 16

identity_filter = np.array([[0, 0, 0],
                           [0, 1, 0],
                           [0, 0, 0]])

emboss_filter = np.array([[-2, -1, 0],
                          [-1, 1, 1],
                          [0, 1, 2]])

box_filter = np.array([[1, 1, 1],
                       [1, 1, 1],
                       [1, 1, 1]]) / 9

custom_filter = np.array([[0, 0, 0],
                          [1, 1, 1],
                          [1, 1, 1]])

filter_names = ['sobel', 'gaussian', 'identity',
                'emboss', 'box', 'custom']

filter_set = [sobel_filter, gaussian_filter, identity_filter,
           emboss_filter, box_filter, custom_filter]

# OLD MODEL:
# model = Sequential([
#     layers.Rescaling(scale=1./255, input_shape=(224, 224, 3)),
#     layers.Conv2D(filters=32, kernel_size=3, activation='relu', padding='same'),
#     layers.Conv2D(filters=75, kernel_size=3, activation='relu', padding='same'),
#     layers.BatchNormalization(),
#     layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'),
#     layers.Dropout(0.2),
#     layers.Conv2D(filters=75, kernel_size=3, activation='relu', padding='same'),
#     layers.MaxPooling2D(pool_size=(2, 2), strides=2, padding='same'),
#     layers.Dropout(0.2),
#     layers.BatchNormalization(),
#     layers.Flatten(),
#     layers.Dense(units=512, activation='relu'),
#     layers.Dense(units=1, activation='sigmoid')
# ])

model = Sequential([
    layers.Rescaling(scale=1./255, input_shape=(224,224,3)),
    layers.Conv2D(filters=32, kernel_size=7),
    layers.MaxPooling2D(),
    layers.BatchNormalization(),
    layers.Dropout(0.2),
    layers.Conv2D(filters=64,  kernel_size=7),
    layers.MaxPooling2D(),
    layers.BatchNormalization(),
    layers.Dropout(0.2),
    layers.Conv2D(filters=128,  kernel_size=7),
    layers.MaxPooling2D(),
    layers.Dropout(0.2),
    layers.Flatten(),
    layers.Dense(units=128, activation="relu",
                 kernel_initializer="he_normal"),
     layers.BatchNormalization(),
    layers.Dropout(0.2),
    layers.Dense(units=64, activation="relu",
                 kernel_initializer="he_normal"),
    layers.Dropout(0.2),
    layers.Dense(units=1, activation="sigmoid")
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

# creating and training the CNN
train_set = np.asarray(train_set)

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)

history = model.fit(x_train, y_train,
                    shuffle=True,
                    batch_size=16,
                    # epochs=50,
                    epochs = 12,
                    verbose=1,
                    callbacks = callback)

accuracy = history.history['accuracy']
loss = history.history['loss']
epochs = range(1, len(loss) + 1)

plt.figure(figsize=(16, 4))

plt.subplot(1, 2, 1)
plt.plot(epochs, accuracy, label='Training Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower left')
plt.title('Training Accuracy')
plt.show()

from sklearn.metrics import accuracy_score

x_test,y_test=retreive_dataset(test_set)

y_pred = model.predict(x_test)
y_test_pred_binary = (y_pred > 0.5).astype(int)

y_train_pred = model.predict(x_train)
y_train_pred_binary = (y_train_pred > 0.5).astype(int)

test_accuracy = accuracy_score(y_test, y_test_pred_binary)
train_accuracy = accuracy_score(y_train, y_train_pred_binary)

print(f"Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"Training Accuracy: {train_accuracy * 100:.2f}%")